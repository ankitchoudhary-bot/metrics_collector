name: LLM Katib Hyperparameter Tuning
description: Runs Katib hyperparameter optimization with HuggingFace + LoRA
inputs: []
outputs: []

implementation:
  container:
    image: python:3.10
    command:
      - sh
      - -c
      - |
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet kubeflow-katib[all] transformers peft || \
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet kubeflow-katib[all] transformers peft --user
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import kubeflow.katib as katib
        from kubeflow.katib import KatibClient
        from transformers import AutoModelForSequenceClassification, TrainingArguments
        from peft import LoraConfig
        from kubeflow.storage_initializer.hugging_face import (
            HuggingFaceModelParams,
            HuggingFaceDatasetParams,
            HuggingFaceTrainerParams,
        )
        from types import SimpleNamespace

        # Model
        hf_model = HuggingFaceModelParams(
            model_uri="hf://meta-llama/Llama-3.2-1B",
            transformer_type=AutoModelForSequenceClassification,
        )

        # Dataset
        hf_dataset = HuggingFaceDatasetParams(
            repo_id="imdb",
            split="train[:1000]",
        )

        # LoRA config as namespace
        class KatibLoraConfig(SimpleNamespace):
            def __init__(self, **kwargs):
                super().__init__(**kwargs)

        lora_cfg = KatibLoraConfig(
            r=katib.search.int(min=8, max=32),
            lora_alpha=8,
            lora_dropout=0.1,
            bias="none",
        )

        # Training params
        hf_tuning_parameters = HuggingFaceTrainerParams(
            training_parameters=TrainingArguments(
                output_dir="results",
                save_strategy="no",
                learning_rate=katib.search.double(min=1e-05, max=5e-05),
                num_train_epochs=3,
            ),
            lora_config=lora_cfg,
        )

        # Katib Client
        cl = KatibClient(namespace="kubeflow")
        exp_name = "llama"

        cl.tune(
            name=exp_name,
            model_provider_parameters=hf_model,
            dataset_provider_parameters=hf_dataset,
            trainer_parameters=hf_tuning_parameters,
            objective_metric_name="train_loss",
            objective_type="minimize",
            algorithm_name="random",
            max_trial_count=1,
            parallel_trial_count=1,
            resources_per_trial=katib.TrainerResources(
                num_workers=1,
                num_procs_per_worker=1,
                resources_per_worker={"gpu": 2, "cpu": 2, "memory": "4G"},
            ),
        )

        # Wait and fetch best hyperparams
        cl.wait_for_experiment_condition(name=exp_name)
        print(cl.get_optimal_hyperparameters(exp_name))
