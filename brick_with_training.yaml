name: Katib Tuner
description: Launches a Katib experiment using an objective function defined in the container.

inputs:
  - name: model_type
    type: String
    description: "Choose one of ['sklearn', 'xgboost', 'pytorch']"

outputs:
  - name: best_hyperparams
    type: JsonArray
    description: Best parameter set

implementation:
  container:
    image: ankitdockerhubprofile/katib-ff-whole:latest
    command:
      - python3
      - -u
      - -c
      - |
        import argparse
        import json
        import os
        from kubernetes import client, config
        import kubeflow.katib as katib
        from kubeflow.katib import (
            V1beta1AlgorithmSpec,
            V1beta1Experiment,
            V1beta1ExperimentSpec,
            V1beta1ObjectiveSpec,
            V1beta1ParameterSpec,
            V1beta1EarlyStoppingSpec,
            V1beta1TrialTemplate,
            V1beta1MetricsCollectorSpec,
            V1beta1CollectorSpec,
            V1beta1FileSystemPath,
        )

        # Load K8s config (cluster)
        # Load K8s config
        try:
            config.load_incluster_config()
        except config.ConfigException:
            config.load_kube_config()


        parser = argparse.ArgumentParser()
        parser.add_argument("--model_type", type=str, required=True)
        parser.add_argument("--best_hyperparams", type=str, required=True)
        args = parser.parse_args()

        model_type = args.model_type.lower()

        # Define Search Spaces
        search_spaces = {
            "sklearn": [
                V1beta1ParameterSpec(name="max_depth", parameter_type="int", feasible_space={"min": "3", "max": "10"}),
                V1beta1ParameterSpec(name="n_estimators", parameter_type="int", feasible_space={"min": "50", "max": "150"}),
            ],
            "xgboost": [
                V1beta1ParameterSpec(name="max_depth", parameter_type="int", feasible_space={"min": "3", "max": "12"}),
                V1beta1ParameterSpec(name="learning_rate", parameter_type="double", feasible_space={"min": "0.01", "max": "0.3"}),
            ],
            "pytorch": [
                V1beta1ParameterSpec(name="lr", parameter_type="double", feasible_space={"min": "0.0001", "max": "0.1"}),
                V1beta1ParameterSpec(name="threshold", parameter_type="double", feasible_space={"min": "0.5", "max": "4"}),
            ]
        }

        if model_type not in search_spaces:
            raise ValueError(f"Unsupported model type: {model_type}")

        metrics_collector = V1beta1MetricsCollectorSpec(
            source={
                "fileSystemPath": V1beta1FileSystemPath(
                    path="/var/log/katib/metrics.log",          #"/output/metrics.log
                    kind="File",          # use string
                    format="JSON"         # use string
                )
            },
            collector={"kind": "File"}
        )


        # Define Katib Experiment Spec
        experiment_name = f"katib-{model_type}-tuning"
        namespace = "admin"

        # Define objective metric and goal
        objective_spec = V1beta1ObjectiveSpec(
            type="maximize",
            goal=0.99,
            objective_metric_name="accuracy"
        )
        # Search space algorithm
        algorithm_spec = V1beta1AlgorithmSpec(algorithm_name="tpe")
        # Early stooping algorithm
        early_stopping_spec = V1beta1EarlyStoppingSpec(algorithm_name="medianstop")

        trial_template = V1beta1TrialTemplate(
            retain=True,  # optional: retain trial pods
            primary_container_name="training-container",
            trial_parameters=[
                {"name": p.name, "description": f"{p.name} value", "reference": p.name}
                for p in search_spaces[model_type]
            ],
            trial_spec={
                "apiVersion": "batch/v1",
                "kind": "Job",
                "spec": {
                    "template": {
                        "spec": {
                            "containers": [
                                {
                                    "name": "training-container",
                                    "image": "ankitdockerhubprofile/katib-ff-whole:latest",
                                    "command": ["python", "objective_fn.py"],
                                    "args": [
                                        "--lr", "${trialParameters.lr}",
                                        "--threshold", "${trialParameters.threshold}"
                                    ],
                                    "resources": {
                                        "limits": {"cpu": "2", "memory": "4Gi"}
                                    }
                                }
                            ],
                            
                            "restartPolicy": "Never"
                        }
                    }
                }
            }
        )

        experiment_spec = V1beta1ExperimentSpec(
            objective=objective_spec,
            algorithm=algorithm_spec,
            parameters=search_spaces[model_type],
            trial_template=trial_template,
            metrics_collector_spec=metrics_collector,
            max_trial_count=4,
            parallel_trial_count=2,
            max_failed_trial_count=2,
            early_stopping=early_stopping_spec
        )

        # Create the Experiment
        katib_client = katib.KatibClient(namespace=namespace)
        experiment = V1beta1Experiment(
            api_version="kubeflow.org/v1beta1",
            kind="Experiment",
            metadata=client.V1ObjectMeta(name=experiment_name, namespace=namespace),
            spec=experiment_spec
        )

        katib_client.create_experiment(experiment)

        # Wait for Experiment Completion
        katib_client.wait_for_experiment_condition(name=experiment_name, namespace=namespace, timeout=3600)

        # Get Optimal Parameters
        best = katib_client.get_optimal_hyperparameters(name=experiment_name, namespace=namespace)
        params = best.parameter_assignments
        hp_dict = {p.name: float(p.value) for p in params}
        print("Best Hyperparameters Found:", hp_dict)

        # Save hyperparameters to file
        dir_path = os.path.dirname(args.best_hyperparams)
        if dir_path:
            os.makedirs(dir_path, exist_ok=True)

        with open(args.best_hyperparams, "w") as f:
            json.dump(hp_dict, f, indent=2)


    args:
      - --model_type
      - {inputValue: model_type}
      - --best_hyperparams
      - {outputPath: best_hyperparams}
